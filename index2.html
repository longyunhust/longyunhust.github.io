<!doctype html>
<html>
  <head>
  <script src="https://use.fontawesome.com/baff6f55f5.js"></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Yunlong Meng</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-29643011-3', 'auto');
      ga('send', 'pageview');
    </script>

    <!-- For all browsers -->
    <link rel="stylesheet" href="assets/css/academicons.min.css"/>
    <link rel="stylesheet" href="assets/css/academicons.css"/>

  </head>

  <body>
    <div class="wrapper">

    <section>
    <h1>Yunlong Meng</h1>
    <!-- <h3>Principle Research Scientist</h3>   -->  
    
    <p>I am currently a principle research scientist at the Shanghai Em-Data Technology. I received my Ph.D. degree in Mechanical 
      Automation and Engineering from The Chinese University of Hong Kong. </p>
    <p>My researches are mainly on computational imaging/computational photography, and computer vision,  including structured 
      illumination, generative adversarial networks, image-to-image translation, and cross-domain object detecton. </p>
    </section>
	  
    
	   
	<section>
    <!-- <h2><a id="published-papers-" class="anchor" href="#publications" aria-hidden="true"><span class="octicon octicon-link"></span></a>Publications</h2>   -->
  <h2>Publications</h2>
  
  </section>
	
	<section>
    <h3>Conference Publications</h3>
	    <!-- ================================================================================================================================== -->
		<b><li>
		<!-- add title below -->
		Unsupervise Image-to-Image translation with Patch Pyramid Dual Contrastive Learning for Cross Domain Detection.  
		</li></b>
		<!-- add authors below -->
		Yunlong Meng<sup>+</sup>, Lifan Zhao<sup>+</sup>, and Lin Xu  &nbsp; &nbsp; (<sup>+</sup>Equal Contribution) 
		<br><i> 
		<!-- add publication name below -->
		International Conference of Multimedia Expro (ICME)  
		</i>,  
		<!-- add published year below -->
		2022
		.<br> 
		<!-- paper related links -->
		<a href="  " target="_blank">PDF</a><br>   &nbsp;&nbsp;&nbsp;&nbsp; 
		<!-- one figure to summarize the published work -->
		<img src="https://github.com/longyunhust/longyunhust.github.io/blob/main/researches/PPDCLUIT.png?raw=true" height="320" width="400"> <br>  
		<p><button class="accordion"> Abstract </button></p> <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> 
		<!-- add abstract below -->
		Unsupervised image-to-image (UI2I) translation methods aim to learn a mapping between different visual domains with well-preserved content 
		and consistent structure. It has been proven that the generated images are useful for enhancing the performance of computer vision tasks 
		like object detection in a different domain with distribution discrepancies. Current UI2I translation techniques are highly relied on 
		cycle-structure consistency and content/style disentanglement. The intense focus has been paid on global translation over an entire 
		image, instead of the stylistic variations and complex structure among multiple disparate object instances and backgrounds, resulting in 
		unsatisfactory performance gain for the downstream vision tasks. In this work, we propose Patch Pyramid Dual Contrastive Learning Unsupervised 
		Image Translation (PPDCLUIT) framework to strengthen the cross domain object detection performance. We present a patch pyramid dual contrastive 
		learning strategy for coherent associations at each specific location and propose identity loss to further enforce the object instances 
		preservation. We implement extensive experiments to demonstrate the efficacy of our PPDCLUIT framework for cross domain object detection and 
		achieve state-of-the-art performance on the benchmarks of Normal-to-Foggy, and Day-to-Night.

		</div></p>
	    <!-- ================================================================================================================================== -->

	    <!-- ================================================================================================================================== -->
		<b><li>
		<!-- add title below -->
		OA-FSUI2IT: A Novel Few-Shot Cross Domain Object Detection Framework with Object-Aware Few-Shot Unsupervised Image-to-Image Translation.
		</li></b>
		<!-- add authors below -->
		Lifan Zhao<sup>+</sup>, Yunlong Meng<sup>+</sup>, and Lin Xu &nbsp; &nbsp; (<sup>+</sup>Equal Contribution)  
		<br><i> 
		<!-- add publication name below -->
		AAAI Conference on Artificial Intelligence (AAAI)  
		</i>,
		<!-- add published year below -->
		2022
		.<br> 
		<!-- paper related links -->
		<a href="https://ojs.aaai.org/index.php/AAAI/article/view/20253" target="_blank">PDF</a> &nbsp;&nbsp;&nbsp;&nbsp;  
		<a href="https://github.com/emdata-ailab/FSCD-Det" target="_blank">Code</a><br> &nbsp;&nbsp;&nbsp;&nbsp;  
		<!-- one figure to summarize the published work -->
		<img src="https://github.com/longyunhust/longyunhust.github.io/blob/main/researches/OA-FSUI2IT.png?raw=true" height="125" width="400"> <br>  
		<p><button class="accordion"> Abstract </button></p> <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> 
		<!-- add abstract below -->
		Unsupervised image-to-image (UI2I) translation methods aim to learn a mapping between different visual domains with well-preserved 
		content and consistent structure. It has been proven that the generated images are quite useful for enhancing the performance of 
		computer vision tasks like object detection in a different domain with distribution discrepancies. Current methods require large 
		amounts of images in both source and target domains for successful translation. However, data collection and annotations in many 
		scenarios are infeasible or even impossible. In this paper, we propose an Object-Aware Few-Shot UI2I Translation (OA-FSUI2IT) framework 
		to address the few-shot cross domain (FSCD) object detection task with limited unlabeled images in the target domain. To this end, we 
		first introduce a discriminator augmentation (DA) module into the OA-FSUI2IT framework for successful few-shot UI2I translation. Then, we 
		present a patch pyramid contrastive learning (PPCL) strategy to further improve the quality of the generated images. Last, we propose 
		a self-supervised content-consistency (SSCC) loss to enforce the content-consistency in the translation. We implement extensive 
		experiments to demonstrate the effectiveness of our OA-FSUI2IT framework for FSCD object detection and achieve state-of-the-art 
		performance on the benchmarks of Normal-to-Foggy, Day-to-Night, and Cross-scene adaptation. The source code of our proposed method 
		is also available at https://github.com/emdata-ailab/FSCD-Det.
		
		</div></p>
	    <!-- ================================================================================================================================== -->

		<!-- ================================================================================================================================== -->
		<b><li>
			<!-- add title below -->
			Multi-step LSTM prediction model for visibility prediction 
			</li></b>
			<!-- add authors below -->
			Yunlong Meng, Fengliang Qi, Heng Zuo, Bo Chen, Xian Yuan, and Yao Xiao
			<br><i> 
			<!-- add publication name below -->
			International Joint Conference on Neural Networks (IJCNN)
			</i>,
			<!-- add published year below -->
			2020 
			.<br> 
			<!-- paper related links -->
			<a href="https://ieeexplore.ieee.org/document/9206744" target="_blank">PDF</a><br> &nbsp;&nbsp;&nbsp;&nbsp;
			<!-- <a href="" >Code (https://github.com/emdata-ailab/FSCD-Det)</a><br> -->
			<!-- one figure to summarize the published work -->
			<img src="https://github.com/longyunhust/longyunhust.github.io/blob/main/researches/visibility-prediction-framework.png?raw=true" height="135" width="400"> <br> 
			<p><button class="accordion"> Abstract </button></p> <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> 
			<!-- add abstract below -->
			In this paper, we present a deep learning framework with attention mechanism for visibility prediction. We firstly formulate visibility 
			prediction as a temporal prediction problem. An encoder-decoder architecture based network is proposed to generate a multi-step 
			prediction. To adaptively focus on different parts of the input and output sequence, we incorporate input attention and temporal attention 
			into the network. Experiments verify the feasibility of the proposed model. We produce state-of-the-art prediction accuracy (68.9%) on the 
			runway visual range prediction in our customized data set collected at observation stations of the airport.
		</div></p>
		<!-- ================================================================================================================================== -->

		<!-- ================================================================================================================================== -->
		<b><li>
			<!-- add title below -->
			Fast Two-Snapshot Structured Illumination for Wide-Field Two-Photon Microscopy with Enhanced Axial Resolution and Signal-to-Noise Ratio  
			</li></b>
			<!-- add authors below -->
			Yunlong Meng, Wei Lin, Jialong Chen, Chenglin Li, and Shih-Chi Chen &nbsp;&nbsp;
			<br><i> 
			<!-- add publication name below -->
			Conference on Lasers and Electro-Optics (CLEO)
			</i>,
			<!-- add published year below -->
			2019 
			.<br> 
			<!-- paper related links -->
			<a href="https://ieeexplore.ieee.org/document/8749995" target="_blank">PDF</a><br> &nbsp;&nbsp;&nbsp;&nbsp;
			<!-- <a href="" >Code (https://github.com/emdata-ailab/FSCD-Det)</a><br> -->
			<!-- one figure to summarize the published work -->
			<img src="https://github.com/longyunhust/longyunhust.github.io/blob/main/researches/Fast-two-snapshot-SNR.jpeg?raw=true" height="260" width="400"> <br> 
			<p><button class="accordion"> Abstract </button></p> <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> 
			<!-- add abstract below -->
			We have developed a fully adaptive fast two-snapshot structured illumination algorithm for fast data acquisition and image reconstruction, which 
			can be used in wide-field two-photon microscopy with enhanced axial resolution (~1.25×) and signal-to-noise ratio.
		</div></p>
		<!-- ================================================================================================================================== -->
			
		<!-- ================================================================================================================================== -->
		<b><li>
			<!-- add title below -->
			Automatic detection and quantitative analysis of cells in the mouse primary motor cortex
			</li></b>
			<!-- add authors below -->
			Yunlong Meng, Yong He, Jingpeng Wu; Shangbin Chen, Anan Li, Hui Gong &nbsp;&nbsp;
			<br><i> 
			<!-- add publication name below -->
			International Conference on Photonics and Imaging in Biology and Medicine (PIBM)
			</i>,
			<!-- add published year below -->
			2019 
			.<br> 
			<!-- paper related links -->
			<a href="https://ui.adsabs.harvard.edu/abs/2014SPIE.9230E..1EM/abstract" target="_blank">PDF</a><br> &nbsp;&nbsp;&nbsp;&nbsp;
			<!-- <a href="" >Code (https://github.com/emdata-ailab/FSCD-Det)</a><br> -->
			<!-- one figure to summarize the published work -->
			<img src="https://github.com/longyunhust/longyunhust.github.io/blob/main/researches/AutomaticCellDet.png?raw=true" height="270" width="400"> <br> 
			<p><button class="accordion"> Abstract </button></p> <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> 
			<!-- add abstract below -->
			Neuronal cells play very important role on metabolism regulation and mechanism control, so cell number is a fundamental determinant of brain 
			function. Combined suitable cell-labeling approaches with recently proposed three-dimensional optical imaging techniques, whole mouse brain 
			coronal sections can be acquired with 1-μm voxel resolution. We have developed a completely automatic pipeline to perform cell centroids 
			detection, and provided three-dimensional quantitative information of cells in the primary motor cortex of C57BL/6 mouse. It involves 
			four principal steps: i) preprocessing; ii) image binarization; iii) cell centroids extraction and contour segmentation; iv) laminar 
			density estimation. Investigations on the presented method reveal promising detection accuracy in terms of recall and precision, with 
			average recall rate 92.1% and average precision rate 86.2%. We also analyze laminar density distribution of cells from pial surface to 
			corpus callosum from the output vectorizations of detected cell centroids in mouse primary motor cortex, and find significant cellular 
			density distribution variations in different layers. This automatic cell centroids detection approach will be beneficial for fast 
			cell-counting and accurate density estimation, as time-consuming and error-prone manual identification is avoided.	
		</div></p>
		<!-- ================================================================================================================================== -->
		
		
	</section>
	
	<section>
	<h2> Journal Publications</h2>
	    <!-- ================================================================================================================================== -->
		<b><li>
		<!-- add title below -->
		Multi-focus microscope with HiLo algorithm for fast 3-D fluorescent imaging &nbsp;&nbsp;
		</li></b>
		<!-- add authors below -->
		Wei Lin, Dongping Wang, Yunlong Meng, Shih-Chi Chen
		<br><i> 
		<!-- add publication name below -->
		PLoS One 
		</i>,
		<!-- add published year below -->
		2019 
		.<br> 
		<!-- paper related links -->
		<a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0222729" target="_blank">PDF</a><br>
		<!-- one figure to summarize the published work -->
		<img src="https://github.com/longyunhust/longyunhust.github.io/blob/main/researches/Multi-focus-Microscope.png?raw=true" height="300" width="400"> <br> 
		<p><button class="accordion"> Abstract </button></p> <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> 
		<!-- add abstract below -->
		In this paper, we present a new multi-focus microscope (MFM) system based on a phase mask and HiLo algorithm, achieving high-speed (20 
		volumes per second), high-resolution, low-noise 3-D fluorescent imaging. During imaging, the emissions from the specimen at nine 
		different depths are simultaneously modulated and focused to different regions on a single CCD chip, i.e., the CCD chip is subdivided 
		into nine regions to record images from the different selected depths. Next, HiLo algorithm is applied to remove the background noises 
		and to form clean 3-D images. To visualize larger volumes, the nine layers are scanned axially, realizing fast 3-D imaging. In the imaging 
		experiments, a mouse kidney sample of ~ 60 × 60 × 16 μm3 is visualized with only 10 raw images, demonstrating substantially enhanced 
		resolution and contrast as well as suppressed background noises. The new method will find important applications in 3-D fluorescent 
		imaging, e.g., recording fast dynamic events at multiple depths in vivo.
		</div></p>
	    <!-- ================================================================================================================================== -->

		<!-- ================================================================================================================================== -->
		<b><li>
		<!-- add title below -->
		Fast two-snapshot structured illumination for temporal focusing microscopy with enhanced axial resolution
		</li></b>
		<!-- add authors below -->
		Yunlong Meng, Wei Lin, Chenglin Li, Shih-chi Chen
		<br><i> 
		<!-- add publication name below -->
		Optics Express
		</i>,
		<!-- add published year below -->
		2019 
		.<br> 
		<!-- paper related links -->
		<a href="https://opg.optica.org/oe/fulltext.cfm?uri=oe-25-19-23109&id=372758" target="_blank">PDF</a><br>
		<!-- one figure to summarize the published work -->
		<img src="https://github.com/longyunhust/longyunhust.github.io/blob/main/researches/Fast-two-snapshot.jpeg?raw=true" height="300" width="400"> <br> 
		<p><button class="accordion"> Abstract </button></p> <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> 
		<!-- add abstract below -->
		We present a new two-snapshot structured light illumination (SLI) reconstruction algorithm for fast image acquisition. The new 
		algorithm, which only requires two mutually π phase-shifted raw structured images, is implemented on a custom-built temporal focusing 
		fluorescence microscope (TFFM) to enhance its axial resolution via a digital micromirror device (DMD). First, the orientation of the 
		modulated sinusoidal fringe patterns is automatically identified via spatial frequency vector detection. Subsequently, the modulated 
		in-focal-plane images are obtained via rotation and subtraction. Lastly, a parallel amplitude demodulation method, derived based on 
		Hilbert transform, is applied to complete the decoding processes. To demonstrate the new SLI algorithm, a TFFM is custom-constructed, where 
		a DMD replaces the generic blazed grating in the system and simultaneously functions as a diffraction grating and a programmable binary 
		mask, generating arbitrary fringe patterns. The experimental results show promising depth-discrimination capability with an axial 
		resolution enhancement factor of 1.25, which matches well with the theoretical estimation, i.e, 1.27. Imaging experiments on pollen 
		grain and mouse kidney samples have been performed. The results indicate that the two-snapshot algorithm presents comparable contrast 
		reconstruction and optical cross-sectioning capability than those adopting the conventional root-mean-square (RMS) reconstruction 
		method. The two-snapshot method can be readily applied to any sinusoidally modulated illumination systems to realize high-speed 3D 
		imaging as less frames are required for each in-focal-plane image restoration, i.e., the image acquisition speed is improved by 2.5 times 
		for any two-photon systems.

		</div></p>
		<!-- ================================================================================================================================== -->

		<!-- ================================================================================================================================== -->
		<b><li>
		<!-- add title below -->
		High-speed 3D imaging based on structured illumination and electrically tunable lens
		</li></b>
		<!-- add authors below -->
		Dongping Wang, Yunlong Meng, Dihan Chen, Yeung Yam, and Shih-Chi Chen
		<br><i> 
		<!-- add publication name below -->
		Chinese Optics Letters  
		</i>,
		<!-- add published year below -->
		2017 
		.<br> 
		<!-- paper related links -->
		<a href="https://opg.optica.org/col/abstract.cfm?uri=col-15-9-090004" target="_blank">PDF</a><br>
		<!-- one figure to summarize the published work -->
		<img src="https://github.com/longyunhust/longyunhust.github.io/blob/main/researches/Structured-light-COL.png?raw=true" height="300" width="400"> <br> 
		<p><button class="accordion"> Abstract </button></p> <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> 
		<!-- add abstract below -->
		In this Letter, we present a high-speed volumetric imaging system based on structured illumination and an electrically tunable 
		lens (ETL), where the ETL performs fast axial scanning at hundreds of Hz. In the system, a digital micro-mirror device (DMD) is 
		utilized to rapidly generate structured images at the focal plane in synchronization with the axial scanning unit. The scanning 
		characteristics of the ETL are investigated theoretically and experimentally. Imaging experiments on pollen samples are performed 
		to verify the optical cross-sectioning and fast axial scanning capabilities. The results show that our system can perform fast 
		axial scanning and three-dimensional (3D) imaging when paired with a high-speed camera, presenting an economic solution for 
		advanced biological imaging applications.

		</div></p>
		<!-- ================================================================================================================================== -->

		<!-- ================================================================================================================================== -->
		<b><li>
		<!-- add title below -->
		An Automated Three-Dimensional Detection and Segmentation Method for Touching Cells by Integrating Concave Points Clustering and Random Walker Algorithm
		</li></b>
		<!-- add authors below -->
		Yong He<sup>+</sup>, Yunlong Meng<sup>+</sup>, Hui Gong, Shangbin Chen, Bin Zhang, Wenxiang Ding, Qingming Luo, Anan Li  &nbsp; &nbsp; (<sup>+</sup>Equal Contribution) 
		<br><i> 
		<!-- add publication name below -->
		PLoS One 
		</i>,
		<!-- add published year below -->
		2014 
		.<br> 
		<!-- paper related links -->
		<a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0104437" target="_blank">PDF</a><br>
		<!-- one figure to summarize the published work -->
		<img src="https://github.com/longyunhust/longyunhust.github.io/blob/main/researches/ConcavePointsCellDetSeg.png?raw=true" height="240" width="400"> <br> 
		<p><button class="accordion"> Abstract </button></p> <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> 
		<!-- add abstract below -->
		Characterizing cytoarchitecture is crucial for understanding brain functions and neural diseases. In neuroanatomy, it is an important 
		task to accurately extract cell populations' centroids and contours. Recent advances have permitted imaging at single cell resolution 
		for an entire mouse brain using the Nissl staining method. However, it is difficult to precisely segment numerous cells, especially 
		those cells touching each other. As presented herein, we have developed an automated three-dimensional detection and segmentation 
		method applied to the Nissl staining data, with the following two key steps: 1) concave points clustering to determine the seed 
		points of touching cells; and 2) random walker segmentation to obtain cell contours. Also, we have evaluated the performance of 
		our proposed method with several mouse brain datasets, which were captured with the micro-optical sectioning tomography imaging 
		system, and the datasets include closely touching cells. Comparing with traditional detection and segmentation methods, our approach 
		shows promising detection accuracy and high robustness.

		</div></p>
		<!-- ================================================================================================================================== -->
	
	</section>
	
	<section>
	<h2> Patents </h2>
	    <!-- ================================================================================================================================== -->
		<b><li>
		<!-- add title below -->
		Method for data acquisition and image processing for reconstructing a super-resolved image.  
		</li></b>
		<!-- add authors below -->
		Shih-Chi Chen, Yunlong Meng, and Jialong Chen
		<br><i> 
		<!-- add publication name below -->
		U.S. Patent 10,909,701 
		</i>,
		<!-- add published year below -->
		issued February 2, 2021.
		.<br> 
		<!-- paper related links -->
		<a href="https://patentimages.storage.googleapis.com/31/0d/7f/2a40b0b1e68c45/US10909701.pdf" target="_blank">PDF</a><br>  
		<!-- one figure to summarize the published work -->		
		<p><button class="accordion"> Abstract </button></p> <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> 
		<!-- add abstract below -->
		The present disclosure discloses a method, a data acquisition and image processing system and a non - transitory machine readable medium for 
    obtaining a super-resolved image of an object . The method comprises : obtaining a plurality of structured images of the object by structured 
    light; determining , from the structured images , modulation information of each structured light that comprises spatial frequency, phase 
    shift and modulation factor ; initializing a sample image of the object according the structured images and initializing structured pattern 
    of each structured light by the corresponding modulation information ; and restoring the image with improved resolution by adjusting the 
    sample image and the structured pattern iteratively. 

		</div></p>
	    <!-- ================================================================================================================================== -->
	
    </section>


	  <section>
      <h2> <p>Teaching</p></h2>
      <b><li>Introduction to Control (Fall 2015).<br> </li></b>         
      <b><li>Advanced Robotics (Fall 2017).</li></b>
      <!--  <p> &nbsp; &nbsp; &nbsp; &nbsp; Advanced Robotics </p>     -->
      </section>
      
      <!--  -->
  
      <section>
      <h2> <p>Announcements</p></h2>
      <p> &nbsp; &nbsp; &nbsp; &nbsp;  If you want to have an internship or seek job opportunity, please contact me. </p> </br>
      <p> &nbsp; &nbsp; &nbsp; &nbsp;  <b>Contact: 021-53086185 </b> </p>  
      <p> &nbsp; &nbsp; &nbsp; &nbsp;   <b>Email: yunlong.cuhk@gmail.com </p></br>
      
        </p>  
        &nbsp; &nbsp; &nbsp; &nbsp;    <a href="https://github.com/longyunhust" target="_blank"><i class="fa fa-fw fa-github-square"></i> GitHub</a> 
        &nbsp; &nbsp; &nbsp; &nbsp;    <a href="https://www.linkedin.com/in/yunlong-meng-33887b7a/" class="author-social" target="_blank"><i class="fa fa-fw fa-linkedin-square"></i> LinkedIn</a> 
        &nbsp; &nbsp; &nbsp; &nbsp;    <a href="https://github.com/longyunhust/longyunhust.github.io/blob/main/cv/cv.pdf">Curriculum Vitae</a>  
		&nbsp; &nbsp; &nbsp; &nbsp;    <a href="https://github.com/longyunhust/longyunhust.github.io/blob/main/cv/cv.pdf">Curriculum Vitae (Chinese)</a>  <br>
    </section>

    </div>
    <script src="javascripts/scale.fix.js"></script>
  </body>
</html>
